{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "batch_size=64\n",
    "lam = 1e-4\n",
    "seed=1101\n",
    "log_interval=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(CAE, self).__init__()\n",
    "\n",
    "\t\tself.fc1 = nn.Linear(784, 400, bias = False) # Encoder\n",
    "\t\tself.fc2 = nn.Linear(400, 784, bias = False) # Decoder\n",
    "\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\tdef encoder(self, x):\n",
    "\t\th1 = self.relu(self.fc1(x.view(-1, 784)))\n",
    "\t\treturn h1\n",
    "\n",
    "\tdef decoder(self,z):\n",
    "\t\th2 = self.sigmoid(self.fc2(z))\n",
    "\t\treturn h2\n",
    "\n",
    "\tdef forward(self, x):\n",
    "            h1 = self.encoder(x)\n",
    "            h2 = self.decoder(h1)\n",
    "            return h1, h2\n",
    "\n",
    "        # Writing data in a grid to check the quality and progress\n",
    "\tdef samples_write(self, x, epoch):\n",
    "\t\t_, samples = self.forward(x)\n",
    "\t\t#pdb.set_trace()\n",
    "\t\tsamples = samples.data.cpu().numpy()[:16]\n",
    "\t\tfig = plt.figure(figsize=(4, 4))\n",
    "\t\tgs = gridspec.GridSpec(4, 4)\n",
    "\t\tgs.update(wspace=0.05, hspace=0.05)\n",
    "\t\tfor i, sample in enumerate(samples):\n",
    "\t\t\tax = plt.subplot(gs[i])\n",
    "\t\t\tplt.axis('off')\n",
    "\t\t\tax.set_xticklabels([])\n",
    "\t\t\tax.set_yticklabels([])\n",
    "\t\t\tax.set_aspect('equal')\n",
    "\t\t\tplt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\t\tif not os.path.exists('out/'):\n",
    "\t\t\tos.makedirs('out/')\n",
    "\t\tplt.savefig('out/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "\t\t#self.c += 1\n",
    "\t\tplt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "mse_loss = nn.BCELoss(size_average = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "?nn.BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(W, x, recons_x, h, lam):\n",
    "    \"\"\"Compute the Contractive AutoEncoder Loss\n",
    "    Evalutes the CAE loss, which is composed as the summation of a Mean\n",
    "    Squared Error and the weighted l2-norm of the Jacobian of the hidden\n",
    "    units with respect to the inputs.\n",
    "    See reference below for an in-depth discussion:\n",
    "      #1: http://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder\n",
    "    Args:\n",
    "        `W` (FloatTensor): (N_hidden x N), where N_hidden and N are the\n",
    "          dimensions of the hidden units and input respectively.\n",
    "        `x` (Variable): the input to the network, with dims (N_batch x N)\n",
    "        recons_x (Variable): the reconstruction of the input, with dims\n",
    "          N_batch x N.\n",
    "        `h` (Variable): the hidden units of the network, with dims\n",
    "          batch_size x N_hidden\n",
    "        `lam` (float): the weight given to the jacobian regulariser term\n",
    "    Returns:\n",
    "        Variable: the (scalar) CAE loss\n",
    "    \"\"\"\n",
    "    mse = mse_loss(recons_x, x)\n",
    "    # Since: W is shape of N_hidden x N. So, we do not need to transpose it as\n",
    "    # opposed to #1\n",
    "    dh = h * (1 - h) # Hadamard product produces size N_batch x N_hidden\n",
    "    # Sum through the input dimension to improve efficiency, as suggested in #1\n",
    "    w_sum = torch.sum(Variable(W)**2, dim=1)\n",
    "    # unsqueeze to avoid issues with torch.mv\n",
    "    w_sum = w_sum.unsqueeze(1) # shape N_hidden x 1\n",
    "    contractive_loss = torch.sum(torch.mm(dh**2, w_sum), 0)\n",
    "    return mse + contractive_loss.mul_(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)    \n",
    "        optimizer.zero_grad()\n",
    "        hidden_representation, recons_x = model(data)\n",
    "\n",
    "        # Get the weights\n",
    "        # model.state_dict().keys()\n",
    "        # change the key by seeing the keys manually.\n",
    "        # (In future I will try to make it automatic)\n",
    "        W = model.state_dict()['fc1.weight']\n",
    "        loss = loss_function(W, data.view(-1, 784), recons_x,\n",
    "                             hidden_representation, lam)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            print('Train epoch: {} [{}/{}({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                  epoch, idx*len(data), len(train_loader.dataset),\n",
    "                  100*idx/len(train_loader),\n",
    "                  loss.data[0]/len(data)))\n",
    "\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "         epoch, train_loss / len(train_loader.dataset)))\n",
    "    model.samples_write(data,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 0 [0/60000(0%)]\t Loss: 543.413208\n",
      "Train epoch: 0 [6400/60000(11%)]\t Loss: 243.411453\n",
      "Train epoch: 0 [12800/60000(21%)]\t Loss: 196.214523\n",
      "Train epoch: 0 [19200/60000(32%)]\t Loss: 187.288391\n",
      "Train epoch: 0 [25600/60000(43%)]\t Loss: 165.572693\n",
      "Train epoch: 0 [32000/60000(53%)]\t Loss: 150.081039\n",
      "Train epoch: 0 [38400/60000(64%)]\t Loss: 145.639923\n",
      "Train epoch: 0 [44800/60000(75%)]\t Loss: 147.546555\n",
      "Train epoch: 0 [51200/60000(85%)]\t Loss: 131.824478\n",
      "Train epoch: 0 [57600/60000(96%)]\t Loss: 126.341087\n",
      "====> Epoch: 0 Average loss: 182.4969\n",
      "Train epoch: 1 [0/60000(0%)]\t Loss: 130.199356\n",
      "Train epoch: 1 [6400/60000(11%)]\t Loss: 123.005585\n",
      "Train epoch: 1 [12800/60000(21%)]\t Loss: 123.797432\n",
      "Train epoch: 1 [19200/60000(32%)]\t Loss: 113.383446\n",
      "Train epoch: 1 [25600/60000(43%)]\t Loss: 115.616768\n",
      "Train epoch: 1 [32000/60000(53%)]\t Loss: 107.598900\n",
      "Train epoch: 1 [38400/60000(64%)]\t Loss: 110.946304\n",
      "Train epoch: 1 [44800/60000(75%)]\t Loss: 103.813957\n",
      "Train epoch: 1 [51200/60000(85%)]\t Loss: 105.038940\n",
      "Train epoch: 1 [57600/60000(96%)]\t Loss: 105.271950\n",
      "====> Epoch: 1 Average loss: 111.0625\n",
      "Train epoch: 2 [0/60000(0%)]\t Loss: 104.557236\n",
      "Train epoch: 2 [6400/60000(11%)]\t Loss: 94.179428\n",
      "Train epoch: 2 [12800/60000(21%)]\t Loss: 97.170990\n",
      "Train epoch: 2 [19200/60000(32%)]\t Loss: 91.638329\n",
      "Train epoch: 2 [25600/60000(43%)]\t Loss: 88.880730\n",
      "Train epoch: 2 [32000/60000(53%)]\t Loss: 90.234612\n",
      "Train epoch: 2 [38400/60000(64%)]\t Loss: 89.090004\n",
      "Train epoch: 2 [44800/60000(75%)]\t Loss: 90.787399\n",
      "Train epoch: 2 [51200/60000(85%)]\t Loss: 82.983826\n",
      "Train epoch: 2 [57600/60000(96%)]\t Loss: 87.836655\n",
      "====> Epoch: 2 Average loss: 91.1326\n",
      "Train epoch: 3 [0/60000(0%)]\t Loss: 80.860313\n",
      "Train epoch: 3 [6400/60000(11%)]\t Loss: 81.103111\n",
      "Train epoch: 3 [12800/60000(21%)]\t Loss: 81.173386\n",
      "Train epoch: 3 [19200/60000(32%)]\t Loss: 81.175591\n",
      "Train epoch: 3 [25600/60000(43%)]\t Loss: 81.884216\n",
      "Train epoch: 3 [32000/60000(53%)]\t Loss: 80.771210\n",
      "Train epoch: 3 [38400/60000(64%)]\t Loss: 78.608658\n",
      "Train epoch: 3 [44800/60000(75%)]\t Loss: 80.685478\n",
      "Train epoch: 3 [51200/60000(85%)]\t Loss: 75.428658\n",
      "Train epoch: 3 [57600/60000(96%)]\t Loss: 75.743980\n",
      "====> Epoch: 3 Average loss: 80.1271\n",
      "Train epoch: 4 [0/60000(0%)]\t Loss: 76.755562\n",
      "Train epoch: 4 [6400/60000(11%)]\t Loss: 80.436745\n",
      "Train epoch: 4 [12800/60000(21%)]\t Loss: 72.436607\n",
      "Train epoch: 4 [19200/60000(32%)]\t Loss: 74.941788\n",
      "Train epoch: 4 [25600/60000(43%)]\t Loss: 72.412300\n",
      "Train epoch: 4 [32000/60000(53%)]\t Loss: 70.246391\n",
      "Train epoch: 4 [38400/60000(64%)]\t Loss: 75.292038\n",
      "Train epoch: 4 [44800/60000(75%)]\t Loss: 72.841690\n",
      "Train epoch: 4 [51200/60000(85%)]\t Loss: 69.312569\n",
      "Train epoch: 4 [57600/60000(96%)]\t Loss: 73.112518\n",
      "====> Epoch: 4 Average loss: 73.3563\n",
      "Train epoch: 5 [0/60000(0%)]\t Loss: 73.243843\n",
      "Train epoch: 5 [6400/60000(11%)]\t Loss: 73.723869\n",
      "Train epoch: 5 [12800/60000(21%)]\t Loss: 71.387810\n",
      "Train epoch: 5 [19200/60000(32%)]\t Loss: 67.130646\n",
      "Train epoch: 5 [25600/60000(43%)]\t Loss: 69.899078\n",
      "Train epoch: 5 [32000/60000(53%)]\t Loss: 64.540916\n",
      "Train epoch: 5 [38400/60000(64%)]\t Loss: 70.587929\n",
      "Train epoch: 5 [44800/60000(75%)]\t Loss: 68.476288\n",
      "Train epoch: 5 [51200/60000(85%)]\t Loss: 66.037025\n",
      "Train epoch: 5 [57600/60000(96%)]\t Loss: 67.417252\n",
      "====> Epoch: 5 Average loss: 68.8181\n",
      "Train epoch: 6 [0/60000(0%)]\t Loss: 65.900360\n",
      "Train epoch: 6 [6400/60000(11%)]\t Loss: 67.926170\n",
      "Train epoch: 6 [12800/60000(21%)]\t Loss: 64.495094\n",
      "Train epoch: 6 [19200/60000(32%)]\t Loss: 68.235306\n",
      "Train epoch: 6 [25600/60000(43%)]\t Loss: 67.194611\n",
      "Train epoch: 6 [32000/60000(53%)]\t Loss: 64.425888\n",
      "Train epoch: 6 [38400/60000(64%)]\t Loss: 66.181305\n",
      "Train epoch: 6 [44800/60000(75%)]\t Loss: 63.625751\n",
      "Train epoch: 6 [51200/60000(85%)]\t Loss: 66.476486\n",
      "Train epoch: 6 [57600/60000(96%)]\t Loss: 64.848938\n",
      "====> Epoch: 6 Average loss: 65.5866\n",
      "Train epoch: 7 [0/60000(0%)]\t Loss: 65.255325\n",
      "Train epoch: 7 [6400/60000(11%)]\t Loss: 64.886757\n",
      "Train epoch: 7 [12800/60000(21%)]\t Loss: 66.205833\n",
      "Train epoch: 7 [19200/60000(32%)]\t Loss: 63.773685\n",
      "Train epoch: 7 [25600/60000(43%)]\t Loss: 65.952713\n",
      "Train epoch: 7 [32000/60000(53%)]\t Loss: 63.656174\n",
      "Train epoch: 7 [38400/60000(64%)]\t Loss: 61.124741\n",
      "Train epoch: 7 [44800/60000(75%)]\t Loss: 62.734131\n",
      "Train epoch: 7 [51200/60000(85%)]\t Loss: 63.623901\n",
      "Train epoch: 7 [57600/60000(96%)]\t Loss: 62.707699\n",
      "====> Epoch: 7 Average loss: 63.1878\n",
      "Train epoch: 8 [0/60000(0%)]\t Loss: 65.079613\n",
      "Train epoch: 8 [6400/60000(11%)]\t Loss: 62.063248\n",
      "Train epoch: 8 [12800/60000(21%)]\t Loss: 62.633511\n",
      "Train epoch: 8 [19200/60000(32%)]\t Loss: 62.369514\n",
      "Train epoch: 8 [25600/60000(43%)]\t Loss: 57.009037\n",
      "Train epoch: 8 [32000/60000(53%)]\t Loss: 60.269806\n",
      "Train epoch: 8 [38400/60000(64%)]\t Loss: 58.464741\n",
      "Train epoch: 8 [44800/60000(75%)]\t Loss: 60.571068\n",
      "Train epoch: 8 [51200/60000(85%)]\t Loss: 62.982025\n",
      "Train epoch: 8 [57600/60000(96%)]\t Loss: 59.771027\n",
      "====> Epoch: 8 Average loss: 61.3775\n",
      "Train epoch: 9 [0/60000(0%)]\t Loss: 60.880810\n",
      "Train epoch: 9 [6400/60000(11%)]\t Loss: 61.546375\n",
      "Train epoch: 9 [12800/60000(21%)]\t Loss: 58.779190\n",
      "Train epoch: 9 [19200/60000(32%)]\t Loss: 58.975807\n",
      "Train epoch: 9 [25600/60000(43%)]\t Loss: 62.489292\n",
      "Train epoch: 9 [32000/60000(53%)]\t Loss: 61.369469\n",
      "Train epoch: 9 [38400/60000(64%)]\t Loss: 57.246288\n",
      "Train epoch: 9 [44800/60000(75%)]\t Loss: 59.945110\n",
      "Train epoch: 9 [51200/60000(85%)]\t Loss: 58.556778\n",
      "Train epoch: 9 [57600/60000(96%)]\t Loss: 58.849251\n",
      "====> Epoch: 9 Average loss: 59.9795\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
